{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training: Phishing Brand Classification\n",
    "\n",
    "This notebook covers the complete model training pipeline for phishing brand classification.\n",
    "\n",
    "## Objectives\n",
    "1. Train a deep learning model for brand classification\n",
    "2. Evaluate model performance with focus on minimizing false positives\n",
    "3. Analyze errors and misclassifications\n",
    "4. Optimize confidence threshold\n",
    "5. Benchmark inference speed\n",
    "6. Generate model interpretability visualizations\n",
    "\n",
    "## Key Metrics\n",
    "- **Accuracy**: Overall correctness\n",
    "- **F1 Score**: Balance of precision and recall\n",
    "- **False Positive Rate for 'others'**: Critical metric - benign sites classified as brands\n",
    "- **Per-class metrics**: Identify problematic classes\n",
    "- **Inference speed**: Important for production deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.data.dataset import PhishingDataset, create_dataloaders\n",
    "from src.data.transforms import AlbumentationsTransform, get_train_transforms, get_val_transforms\n",
    "from src.models.classifier import BrandClassifier, create_model\n",
    "from src.models.losses import FocalLoss\n",
    "from src.utils.metrics import (\n",
    "    calculate_metrics,\n",
    "    compute_confusion_matrix,\n",
    "    find_optimal_threshold,\n",
    "    get_false_positive_analysis,\n",
    "    evaluate_with_rejection,\n",
    ")\n",
    "from src.utils.visualization import (\n",
    "    plot_confusion_matrix,\n",
    "    plot_training_curves,\n",
    "    plot_false_positive_analysis,\n",
    "    plot_per_class_metrics,\n",
    "    plot_confidence_distribution,\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = project_root / 'data' / 'raw'\n",
    "PROCESSED_DIR = project_root / 'data' / 'processed'\n",
    "OUTPUT_DIR = project_root / 'outputs'\n",
    "FIGURES_DIR = OUTPUT_DIR / 'figures'\n",
    "MODELS_DIR = OUTPUT_DIR / 'models'\n",
    "\n",
    "for dir_path in [FIGURES_DIR, MODELS_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "CONFIG = {\n",
    "    # Data\n",
    "    'image_size': 224,\n",
    "    'batch_size': 32,\n",
    "    'num_workers': 4,\n",
    "    \n",
    "    # Model\n",
    "    'architecture': 'efficientnet_b0',  # Options: resnet50, efficientnet_b0, efficientnet_b3\n",
    "    'pretrained': True,\n",
    "    'dropout': 0.3,\n",
    "    \n",
    "    # Training\n",
    "    'num_epochs': 30,\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 1e-4,\n",
    "    'use_amp': True,  # Mixed precision training\n",
    "    \n",
    "    # Loss\n",
    "    'use_focal_loss': True,\n",
    "    'focal_gamma': 2.0,\n",
    "    'use_class_weights': True,\n",
    "    \n",
    "    # Evaluation\n",
    "    'confidence_threshold': 0.85,\n",
    "    \n",
    "    # Random seed\n",
    "    'seed': 42,\n",
    "}\n",
    "\n",
    "# Class names\n",
    "CLASS_NAMES = [\n",
    "    'amazon', 'apple', 'facebook', 'google', 'instagram',\n",
    "    'linkedin', 'microsoft', 'netflix', 'paypal', 'twitter',\n",
    "    'others'\n",
    "]\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "OTHERS_IDX = CLASS_NAMES.index('others')\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"\\nNumber of classes: {NUM_CLASSES}\")\n",
    "print(f\"Others class index: {OTHERS_IDX}\")\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "np.random.seed(CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed splits\n",
    "train_df = pd.read_csv(PROCESSED_DIR / 'train.csv')\n",
    "val_df = pd.read_csv(PROCESSED_DIR / 'val.csv')\n",
    "test_df = pd.read_csv(PROCESSED_DIR / 'test.csv')\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# Create transforms\n",
    "train_transform = AlbumentationsTransform(\n",
    "    get_train_transforms(image_size=CONFIG['image_size'])\n",
    ")\n",
    "val_transform = AlbumentationsTransform(\n",
    "    get_val_transforms(image_size=CONFIG['image_size'])\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader, val_loader, test_loader, class_names = create_dataloaders(\n",
    "    train_df=train_df,\n",
    "    val_df=val_df,\n",
    "    test_df=test_df,\n",
    "    data_dir=str(DATA_DIR),\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    use_weighted_sampler=CONFIG['use_class_weights'],\n",
    "    class_names=CLASS_NAMES,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataloaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = create_model(\n",
    "    architecture=CONFIG['architecture'],\n",
    "    num_classes=NUM_CLASSES,\n",
    "    pretrained=CONFIG['pretrained'],\n",
    "    dropout=CONFIG['dropout'],\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model: {CONFIG['architecture']}\")\n",
    "print(f\"Feature dimension: {model.feature_dim}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup class weights\n",
    "train_dataset = train_loader.dataset\n",
    "class_weights = train_dataset.get_class_weights().to(device)\n",
    "\n",
    "print(\"Class weights:\")\n",
    "for name, weight in zip(CLASS_NAMES, class_weights):\n",
    "    marker = \" <-- benign\" if name == 'others' else \"\"\n",
    "    print(f\"  {name}: {weight:.3f}{marker}\")\n",
    "\n",
    "# Create loss function\n",
    "if CONFIG['use_focal_loss']:\n",
    "    criterion = FocalLoss(\n",
    "        alpha=class_weights if CONFIG['use_class_weights'] else None,\n",
    "        gamma=CONFIG['focal_gamma'],\n",
    "    )\n",
    "    print(f\"\\nUsing Focal Loss (gamma={CONFIG['focal_gamma']})\")\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss(\n",
    "        weight=class_weights if CONFIG['use_class_weights'] else None\n",
    "    )\n",
    "    print(\"\\nUsing Cross Entropy Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay'],\n",
    ")\n",
    "\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=CONFIG['num_epochs'],\n",
    "    eta_min=CONFIG['learning_rate'] / 100,\n",
    ")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler() if CONFIG['use_amp'] and device.type == 'cuda' else None\n",
    "\n",
    "print(f\"Optimizer: AdamW (lr={CONFIG['learning_rate']})\")\n",
    "print(f\"Scheduler: CosineAnnealing\")\n",
    "print(f\"Mixed precision: {scaler is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device, scaler=None):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training', leave=False)\n",
    "    for images, labels, _ in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if scaler is not None:\n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = running_loss / len(loader)\n",
    "    accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels, _ in tqdm(loader, desc='Validating', leave=False):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "            \n",
    "            all_probs.extend(probs)\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = running_loss / len(loader)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    accuracy = np.mean(all_preds == all_labels)\n",
    "    \n",
    "    return avg_loss, accuracy, all_labels, all_preds, all_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "history = {\n",
    "    'train_loss': [], 'val_loss': [],\n",
    "    'train_acc': [], 'val_acc': [],\n",
    "    'learning_rate': []\n",
    "}\n",
    "\n",
    "best_val_acc = 0\n",
    "best_epoch = 0\n",
    "experiment_name = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "checkpoint_dir = MODELS_DIR / experiment_name\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Training for {CONFIG['num_epochs']} epochs...\")\n",
    "print(f\"Checkpoints will be saved to: {checkpoint_dir}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(1, CONFIG['num_epochs'] + 1):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, device, scaler\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_labels, val_preds, val_probs = validate(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['learning_rate'].append(current_lr)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch:3d}/{CONFIG['num_epochs']} | \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f} | \"\n",
    "          f\"LR: {current_lr:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'class_names': CLASS_NAMES,\n",
    "            'config': CONFIG,\n",
    "        }, checkpoint_dir / 'best_model.pt')\n",
    "        print(f\"  >> Saved best model (val_acc: {val_acc:.4f})\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Training complete! Best val_acc: {best_val_acc:.4f} at epoch {best_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig = plot_training_curves(history, figsize=(14, 4))\n",
    "plt.savefig(FIGURES_DIR / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(checkpoint_dir / 'best_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc, test_labels, test_preds, test_probs = validate(\n",
    "    model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Loss: {test_loss:.4f}\")\n",
    "print(f\"  Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive metrics\n",
    "metrics = calculate_metrics(\n",
    "    y_true=test_labels,\n",
    "    y_pred=test_preds,\n",
    "    y_proba=test_probs,\n",
    "    class_names=CLASS_NAMES,\n",
    ")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(metrics['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = compute_confusion_matrix(test_labels, test_preds)\n",
    "fig = plot_confusion_matrix(cm, CLASS_NAMES, normalize=True, figsize=(12, 10))\n",
    "plt.savefig(FIGURES_DIR / 'confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class metrics visualization\n",
    "fig = plot_per_class_metrics(metrics, CLASS_NAMES)\n",
    "plt.savefig(FIGURES_DIR / 'per_class_metrics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. False Positive Analysis\n",
    "\n",
    "Critical analysis: How often are benign websites ('others') misclassified as target brands?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False positive analysis for 'others' class\n",
    "fp_analysis = get_false_positive_analysis(\n",
    "    y_true=test_labels,\n",
    "    y_pred=test_preds,\n",
    "    y_proba=test_probs,\n",
    "    others_class_idx=OTHERS_IDX,\n",
    "    class_names=CLASS_NAMES,\n",
    ")\n",
    "\n",
    "print(\"False Positive Analysis for 'Others' (Benign) Class:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total 'others' samples: {fp_analysis['total_others_samples']}\")\n",
    "print(f\"False positives: {fp_analysis['false_positive_count']}\")\n",
    "print(f\"False positive rate: {fp_analysis['false_positive_rate']:.2%}\")\n",
    "print(f\"\\nBrands 'others' was misclassified as:\")\n",
    "for brand, count in sorted(fp_analysis['brand_misclassification_counts'].items(), key=lambda x: -x[1]):\n",
    "    if count > 0:\n",
    "        print(f\"  {brand}: {count}\")\n",
    "\n",
    "# Plot\n",
    "fig = plot_false_positive_analysis(fp_analysis)\n",
    "plt.savefig(FIGURES_DIR / 'false_positive_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Confidence Threshold Optimization\n",
    "\n",
    "Find the optimal confidence threshold that minimizes false positives while maintaining good accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal threshold\n",
    "optimal_threshold, threshold_metrics = find_optimal_threshold(\n",
    "    y_true=test_labels,\n",
    "    y_proba=test_probs,\n",
    "    others_class_idx=OTHERS_IDX,\n",
    "    metric='f1_weighted',\n",
    "    max_fp_rate=0.05,  # Maximum 5% false positive rate for 'others'\n",
    ")\n",
    "\n",
    "print(f\"Optimal Confidence Threshold: {optimal_threshold:.2f}\")\n",
    "print(f\"\\nMetrics at optimal threshold:\")\n",
    "for key, value in threshold_metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with rejection (low-confidence predictions classified as 'others')\n",
    "rejection_results = evaluate_with_rejection(\n",
    "    y_true=test_labels,\n",
    "    y_proba=test_probs,\n",
    "    confidence_threshold=optimal_threshold,\n",
    ")\n",
    "\n",
    "print(f\"\\nEvaluation with Confidence Threshold ({optimal_threshold:.2f}):\")\n",
    "print(f\"  Total samples: {rejection_results['total_samples']}\")\n",
    "print(f\"  Accepted: {rejection_results['accepted_samples']} ({1-rejection_results['rejection_rate']:.1%})\")\n",
    "print(f\"  Rejected: {rejection_results['rejected_samples']} ({rejection_results['rejection_rate']:.1%})\")\n",
    "print(f\"  Accuracy on accepted: {rejection_results['accepted_accuracy']:.4f}\")\n",
    "print(f\"  F1 on accepted: {rejection_results['accepted_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence distribution analysis\n",
    "confidences = test_probs.max(axis=1)\n",
    "correct_mask = test_preds == test_labels\n",
    "\n",
    "fig = plot_confidence_distribution(confidences, correct_mask)\n",
    "plt.savefig(FIGURES_DIR / 'confidence_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConfidence Statistics:\")\n",
    "print(f\"  Correct predictions - Mean: {confidences[correct_mask].mean():.3f}, Std: {confidences[correct_mask].std():.3f}\")\n",
    "print(f\"  Incorrect predictions - Mean: {confidences[~correct_mask].mean():.3f}, Std: {confidences[~correct_mask].std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify misclassified samples\n",
    "test_dataset = test_loader.dataset\n",
    "misclassified_indices = np.where(test_preds != test_labels)[0]\n",
    "\n",
    "print(f\"Total misclassified samples: {len(misclassified_indices)}\")\n",
    "print(f\"Error rate: {len(misclassified_indices) / len(test_labels):.2%}\")\n",
    "\n",
    "# Analyze error patterns\n",
    "error_df = pd.DataFrame({\n",
    "    'true_label': [CLASS_NAMES[test_labels[i]] for i in misclassified_indices],\n",
    "    'pred_label': [CLASS_NAMES[test_preds[i]] for i in misclassified_indices],\n",
    "    'confidence': [test_probs[i].max() for i in misclassified_indices],\n",
    "})\n",
    "\n",
    "print(f\"\\nMost common error patterns:\")\n",
    "error_patterns = error_df.groupby(['true_label', 'pred_label']).size().sort_values(ascending=False)\n",
    "print(error_patterns.head(10).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some misclassified samples\n",
    "from src.data.transforms import denormalize\n",
    "\n",
    "n_samples = min(8, len(misclassified_indices))\n",
    "sample_indices = np.random.choice(misclassified_indices, n_samples, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, idx in zip(axes, sample_indices):\n",
    "    # Get image\n",
    "    img_tensor, label, img_path = test_dataset[idx]\n",
    "    img = denormalize(img_tensor).permute(1, 2, 0).numpy()\n",
    "    \n",
    "    true_label = CLASS_NAMES[label]\n",
    "    pred_label = CLASS_NAMES[test_preds[idx]]\n",
    "    conf = test_probs[idx].max()\n",
    "    \n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"True: {true_label}\\nPred: {pred_label}\\nConf: {conf:.2f}\", \n",
    "                fontsize=9, color='red')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Misclassified Samples', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'misclassified_samples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference Speed Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.predict import PhishingClassifier\n",
    "\n",
    "# Create inference wrapper\n",
    "classifier = PhishingClassifier(\n",
    "    checkpoint_path=str(checkpoint_dir / 'best_model.pt'),\n",
    "    device=str(device),\n",
    "    confidence_threshold=optimal_threshold,\n",
    ")\n",
    "\n",
    "# Get a sample image for benchmarking\n",
    "sample_path = test_df['image_path'].iloc[0]\n",
    "\n",
    "# Run benchmark\n",
    "benchmark = classifier.benchmark_inference_speed(\n",
    "    sample_path,\n",
    "    num_iterations=100,\n",
    "    warmup_iterations=10,\n",
    ")\n",
    "\n",
    "print(\"Inference Speed Benchmark\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Device: {benchmark['device']}\")\n",
    "print(f\"Iterations: {benchmark['num_iterations']}\")\n",
    "print(f\"\\nLatency (ms):\")\n",
    "print(f\"  Mean: {benchmark['mean_latency_ms']:.2f}\")\n",
    "print(f\"  Std:  {benchmark['std_latency_ms']:.2f}\")\n",
    "print(f\"  P50:  {benchmark['p50_latency_ms']:.2f}\")\n",
    "print(f\"  P95:  {benchmark['p95_latency_ms']:.2f}\")\n",
    "print(f\"  P99:  {benchmark['p99_latency_ms']:.2f}\")\n",
    "print(f\"\\nThroughput: {benchmark['throughput_fps']:.1f} FPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Interpretability (GradCAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.interpretability import ModelExplainer\n",
    "\n",
    "# Create explainer\n",
    "explainer = ModelExplainer(\n",
    "    model=model,\n",
    "    class_names=CLASS_NAMES,\n",
    "    image_size=CONFIG['image_size'],\n",
    "    device=str(device),\n",
    ")\n",
    "\n",
    "# Generate explanations for a few samples\n",
    "n_explain = 4\n",
    "sample_paths = test_df.sample(n_explain)['image_path'].tolist()\n",
    "\n",
    "for i, img_path in enumerate(sample_paths):\n",
    "    print(f\"\\nExplanation for sample {i+1}:\")\n",
    "    explanation = explainer.explain(img_path, methods=['gradcam'])\n",
    "    fig = explainer.plot_explanation(explanation)\n",
    "    plt.savefig(FIGURES_DIR / f'gradcam_sample_{i+1}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "results = {\n",
    "    'config': CONFIG,\n",
    "    'training': {\n",
    "        'best_epoch': best_epoch,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'history': history,\n",
    "    },\n",
    "    'test_metrics': {\n",
    "        'accuracy': float(test_acc),\n",
    "        'loss': float(test_loss),\n",
    "        'precision': float(metrics['precision']),\n",
    "        'recall': float(metrics['recall']),\n",
    "        'f1_score': float(metrics['f1_score']),\n",
    "    },\n",
    "    'false_positive_analysis': fp_analysis,\n",
    "    'optimal_threshold': optimal_threshold,\n",
    "    'threshold_metrics': threshold_metrics,\n",
    "    'rejection_results': rejection_results,\n",
    "    'benchmark': benchmark,\n",
    "    'class_names': CLASS_NAMES,\n",
    "}\n",
    "\n",
    "with open(checkpoint_dir / 'results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2, default=lambda x: x.tolist() if hasattr(x, 'tolist') else str(x))\n",
    "\n",
    "print(f\"Results saved to: {checkpoint_dir / 'results.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel: {CONFIG['architecture']}\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f} (epoch {best_epoch})\")\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Accuracy: {test_acc:.4f}\")\n",
    "print(f\"  F1 Score: {metrics['f1_score']:.4f}\")\n",
    "print(f\"\\nFalse Positive Rate ('others'): {fp_analysis['false_positive_rate']:.2%}\")\n",
    "print(f\"Optimal Confidence Threshold: {optimal_threshold:.2f}\")\n",
    "print(f\"\\nInference Speed: {benchmark['mean_latency_ms']:.2f} ms ({benchmark['throughput_fps']:.1f} FPS)\")\n",
    "print(f\"\\nModel saved to: {checkpoint_dir / 'best_model.pt'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
